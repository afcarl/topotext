{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting word paths (sequences of embeddings)\n",
    "\n",
    "First, download to the same directory as this note the data from the [Yelp recruiting contest](https://www.kaggle.com/c/yelp-recruiting) on [kaggle](https://www.kaggle.com/):\n",
    "* https://www.kaggle.com/c/yelp-recruiting/download/yelp_training_set.zip\n",
    "* https://www.kaggle.com/c/yelp-recruiting/download/yelp_test_set.zip\n",
    "\n",
    "You'll need to sign-up for kaggle.\n",
    "\n",
    "Then we define below a super simple parser, and a generator for sentences from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contractions = re.compile(r\"'s*|-|\\\"\")\n",
    "# all non alphanumeric\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "# some stops to remove\n",
    "stops = re.compile(r'(\\s[,:\\)\\(]\\s)')\n",
    "# for sentence splitter\n",
    "alteos = re.compile(r'([!\\?])')\n",
    "\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = symbols.sub(r' \\1 ', text)\n",
    "    text = stops.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    text = alteos.sub(r' \\1 .', text)\n",
    "    return text\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "\n",
    "def YelpSentences(label, stars=[1,2,3,4,5]):\n",
    "    with ZipFile(\"yelp_%s_set.zip\"%label, 'r') as zf:\n",
    "        with zf.open(\"yelp_%s_set/yelp_%s_set_review.json\"%(label,label)) as f:\n",
    "            for line in f:\n",
    "                rev = json.loads(line)\n",
    "                if rev['stars'] in stars:\n",
    "                    text = rev['text'].clean()\n",
    "                    for s in text.split(\".\"):\n",
    "                        yield s.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small dataset, bring everything into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = list(YelpSentences(\"training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use gensim to train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## create a w2v learner \n",
    "w2v = Word2Vec(sentences, workers=8, iter=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to save vector path to file.\n",
    "\n",
    "Each row is a word, sentence reads from top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# (throws a key error if words are not in the vocab.)\n",
    "def savepath(words):\n",
    "    print(words)\n",
    "    np.savetxt(\"sentences/\"+\"_\".join(words)+\".txt\", w2v[words], fmt=\"%.6f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cherry pick a few and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'wife', u'took', u'me', u'here', u'on', u'my', u'birthday', u'for', u'breakfast', u'and', u'it', u'was', u'excellent']\n"
     ]
    }
   ],
   "source": [
    "savepath(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'horchata', u'is', u'handmade', u'and', u'delicious']\n"
     ]
    }
   ],
   "source": [
    "savepath(sentences[97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badsentences = list(YelpSentences(\"test\", [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'we', u'thought', u'this', u'was', u'a', u'little', u'strange', u'since', u'every', u'single', u'other', u'kennel', u'weve', u'ever', u'been', u'to', u'was', u'willing', u'and', u'wanted', u'to', u'give', u'us', u'a', u'tour']\n"
     ]
    }
   ],
   "source": [
    "savepath(badsentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'thanks', u'for', u'lying', u'to', u'my', u'face', u'dude']\n"
     ]
    }
   ],
   "source": [
    "savepath(badsentences[800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
